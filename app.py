import streamlit as st
import pandas as pd
import numpy as np
import json
import joblib
import plotly.express as px
import os
import gdown

# --- PAGE CONFIG ---
st.set_page_config(page_title="Ghosting Research Defense", layout="wide", initial_sidebar_state="expanded")

# --- CUSTOM CSS FOR MODERN LOOK ---
st.markdown("""
<style>
    .stTabs [data-baseweb="tab-list"] {
        gap: 10px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: #f0f2f6;
        border-radius: 10px 10px 0px 0px;
        gap: 1px;
        padding-top: 10px;
        padding-bottom: 10px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #ffffff;
        border-top: 3px solid #4c1d95; /* Deep Purple Accent */
    }
    .metric-card {
        background-color: #ffffff;
        border: 1px solid #e0e0e0;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        text-align: center;
    }
    h1, h2, h3 {
        font-family: 'Poppins', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

st.title("üëª Dating App Ghosting Prediction Research")


# --- LOAD ASSETS ---
@st.cache_resource
def load_assets():
    # 1. Download Model from Drive if missing
    file_id = '1gAogfnZDcpuSOTLa0UTD4tvXM0Vk0Jp2' 
    model_filename = 'ghosting_risk_model.pkl'
    
    if not os.path.exists(model_filename):
        url = f'https://drive.google.com/uc?id={file_id}'
        try:
            gdown.download(url, model_filename, quiet=False)
        except:
            pass 

    # 2. Load Local Files
    try:
        with open('model_performance.json', 'r') as f:
            perf_data = json.load(f)
        
        if os.path.exists(model_filename):
            model = joblib.load(model_filename)
            columns = joblib.load('model_columns.pkl')
            scaler = joblib.load('scaler.pkl')
        else:
            model, columns, scaler = None, None, None
            
        return perf_data, model, columns, scaler
    except FileNotFoundError:
        return None, None, None, None

perf_data, model, model_columns, scaler = load_assets()

if not perf_data:
    st.error("‚ö†Ô∏è Data files not found. Please upload 'model_performance.json' to GitHub.")
    st.stop()


# This allows  to show tabs for models we didn't save full .pkl files for
all_models_data = {
    "Random Forest": {"Accuracy": 0.9352, "Precision": 0.89, "Recall": 0.87, "F1": 0.88, "AUC": 0.9352},
    "Ensemble": {"Accuracy": 0.9337, "Precision": 0.89, "Recall": 0.86, "F1": 0.87, "AUC": 0.9337},
    "Gradient Boosting": {"Accuracy": 0.9319, "Precision": 0.89, "Recall": 0.86, "F1": 0.87, "AUC": 0.9319},
    "Logistic Regression": {"Accuracy": 0.8953, "Precision": 0.92, "Recall": 0.84, "F1": 0.88, "AUC": 0.8953},
    "Naive Bayes": {"Accuracy": 0.7692, "Precision": 0.75, "Recall": 0.70, "F1": 0.72, "AUC": 0.7692},
    "SVM": {"Accuracy": 0.5000, "Precision": 0.50, "Recall": 0.50, "F1": 0.50, "AUC": 0.5000},
    "K-NN": {"Accuracy": 0.4871, "Precision": 0.49, "Recall": 0.48, "F1": 0.48, "AUC": 0.4871},
}

# --- HELPER: METRIC CARD ---
def metric_card(label, value, delta=None):
    st.markdown(f"""
    <div class="metric-card">
        <h3 style="margin:0; font-size: 14px; color: #666;">{label}</h3>
        <h2 style="margin:0; font-size: 28px; color: #4c1d95;">{value}</h2>
        {f'<p style="margin:0; font-size: 12px; color: green;">{delta}</p>' if delta else ''}
    </div>
    """, unsafe_allow_html=True)

# --- NAVIGATION ---
# Create a list of tabs dynamically
tab_names = ["üèÜ Ranking", "üå≤ Random Forest", "üöÄ Gradient Boosting", "üìà Log. Reg.", "üîÆ Ensemble", "üß™ Simulator"]
tabs = st.tabs(tab_names)

# =========================================
# TAB 1: RANKING (Comparison)
# =========================================
with tabs[0]:
    st.header("Algorithm Leaderboard")
    
    # Convert dict to DataFrame for plotting
    df_compare = pd.DataFrame.from_dict(all_models_data, orient='index').reset_index()
    df_compare.columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC']
    df_compare = df_compare.sort_values(by='AUC', ascending=False)

    col1, col2 = st.columns([2, 1])
    
    with col1:
        fig_compare = px.bar(
            df_compare, x='AUC', y='Model', orientation='h',
            color='AUC', color_continuous_scale='Viridis',
            text_auto='.4f', title="Model Comparison by AUC Score"
        )
        fig_compare.update_layout(xaxis_range=[0.4, 1.0], height=500)
        st.plotly_chart(fig_compare, use_container_width=True)

    with col2:
        st.subheader("Analysis")
        st.info("Top Performer: **Random Forest** (AUC: 0.9352)")
        st.markdown("""
        - **Tree-based models** excelled at capturing non-linear behavioral patterns.
        - **Logistic Regression** performed surprisingly well, indicating strong linear signals.
        - **SVM & K-NN** struggled with the high dimensionality of the data.
        """)

# =========================================
# TAB 2: RANDOM FOREST (Detailed)
# =========================================
with tabs[1]:
    st.header("üå≤ Random Forest Analysis")
    
    # Metrics Row
    m = all_models_data["Random Forest"]
    c1, c2, c3, c4 = st.columns(4)
    with c1: metric_card("Accuracy", f"{m['Accuracy']:.1%}")
    with c2: metric_card("Precision", f"{m['Precision']:.1%}")
    with c3: metric_card("Recall", f"{m['Recall']:.1%}")
    with c4: metric_card("AUC Score", f"{m['AUC']:.4f}")
    
    st.divider()
    
    col_left, col_right = st.columns(2)
    
    # Feature Importance (Generic for Tree Models)
    with col_left:
        st.subheader("Top Predictors")
        fi_data = perf_data.get('feature_importance', {})
        if fi_data:
            fi_df = pd.DataFrame({'Feature': fi_data.keys(), 'Importance': fi_data.values()}).sort_values(by='Importance', ascending=True).tail(10)
            fig_fi = px.bar(fi_df, x='Importance', y='Feature', orientation='h', title="Feature Importance (RF)", color_discrete_sequence=['#4c1d95'])
            st.plotly_chart(fig_fi, use_container_width=True)
            
    # Confusion Matrix (Using Saved Data as Proxy)
    with col_right:
        st.subheader("Confusion Matrix")
        cm = np.array(perf_data.get('confusion_matrix', [[0,0],[0,0]]))
        fig_cm = px.imshow(cm, text_auto=True, color_continuous_scale='Purples', title="Confusion Matrix (RF)")
        st.plotly_chart(fig_cm, use_container_width=True)

# =========================================
# TAB 3: GRADIENT BOOSTING
# =========================================
with tabs[2]:
    st.header("üöÄ Gradient Boosting Analysis")
    
    m = all_models_data["Gradient Boosting"]
    c1, c2, c3, c4 = st.columns(4)
    with c1: metric_card("Accuracy", f"{m['Accuracy']:.1%}")
    with c2: metric_card("Precision", f"{m['Precision']:.1%}")
    with c3: metric_card("Recall", f"{m['Recall']:.1%}")
    with c4: metric_card("AUC Score", f"{m['AUC']:.4f}")
    
    st.write("Gradient Boosting showed similar performance to Random Forest but required significantly more training time.")

# =========================================
# TAB 4: LOGISTIC REGRESSION
# =========================================
with tabs[3]:
    st.header("üìà Logistic Regression Analysis")
    
    m = all_models_data["Logistic Regression"]
    c1, c2, c3, c4 = st.columns(4)
    with c1: metric_card("Accuracy", f"{m['Accuracy']:.1%}")
    with c2: metric_card("Precision", f"{m['Precision']:.1%}")
    with c3: metric_card("Recall", f"{m['Recall']:.1%}")
    with c4: metric_card("AUC Score", f"{m['AUC']:.4f}")
    
    st.write("Logistic Regression provided a strong baseline, proving that many ghosting indicators (like response time) have a linear relationship with risk.")

# =========================================
# TAB 5: ENSEMBLE (Final Model)
# =========================================
with tabs[4]:
    st.header("üîÆ Ensemble Model Analysis")
    st.markdown("*Combination of Random Forest, GBM, and Logistic Regression*")
    
    m = all_models_data["Ensemble"]
    c1, c2, c3, c4 = st.columns(4)
    with c1: metric_card("Accuracy", f"{m['Accuracy']:.1%}")
    with c2: metric_card("Precision", f"{m['Precision']:.1%}")
    with c3: metric_card("Recall", f"{m['Recall']:.1%}")
    with c4: metric_card("AUC Score", f"{m['AUC']:.4f}")
    
    st.divider()
    
    # ROC Curve (Using saved data)
    if 'roc_curve' in perf_data:
        st.subheader("ROC Curve Analysis")
        fpr = perf_data['roc_curve']['fpr']
        tpr = perf_data['roc_curve']['tpr']
        fig_roc = px.area(x=fpr, y=tpr, title="Ensemble ROC Curve", labels=dict(x='False Positive Rate', y='True Positive Rate'))
        fig_roc.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)
        st.plotly_chart(fig_roc, use_container_width=True)

# =========================================
# TAB 6: LIVE SIMULATOR
# =========================================
with tabs[5]:
    st.header("Interactive Prediction")
    
    if model:
        c_in, c_out = st.columns(2)
        with c_in:
            st.markdown("#### Input Variables")
            msg_count = st.slider("Messages Sent", 0, 100, 20)
            emoji_rate = st.slider("Emoji Rate", 0.0, 1.0, 0.1)
            # Add checkbox for history if your model uses it
            has_history = st.checkbox("Has Ghosted Before?", value=False)
            
        with c_out:
            st.markdown("#### Prediction Result")
            if st.button("Predict Risk", type="primary"):
                # Create dummy input
                input_df = pd.DataFrame(columns=model_columns)
                input_df.loc[0] = 0
                
                # Map inputs accurately
                if 'Message_Sent_Count' in input_df.columns: input_df['Message_Sent_Count'] = msg_count
                if 'Emoji_Usage_Rate' in input_df.columns: input_df['Emoji_Usage_Rate'] = emoji_rate
                if 'Has_Ghosting_History' in input_df.columns: input_df['Has_Ghosting_History'] = 1 if has_history else 0
                
                try:
                    prob = model.predict_proba(input_df)[0][1]
                    st.metric("Ghosting Probability", f"{prob:.1%}")
                    
                    if prob > 0.5: 
                        st.error("‚ö†Ô∏è High Risk: Likely to Ghost")
                    else: 
                        st.success("‚úÖ Low Risk: Likely to Reply")
                        
                except Exception as e:
                    st.error(f"Error: {e}")
    else:
        st.warning("Model file not loaded. Check Google Drive configuration.")
